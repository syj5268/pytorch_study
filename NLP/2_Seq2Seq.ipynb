{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    text = re.sub(r'([^\\w\\s])', r' \\1 ', text) # replace punctuation with space\n",
    "    text = re.sub(r'\\s+', ' ', text) # replace multiple spaces with single space\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexsFromSentence(vocab, sentence):\n",
    "    return [vocab.get(word, vocab['<UNK>']) for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(vocab, sentence):\n",
    "    indexes = indexsFromSentence(vocab, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device = device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "UNK_token = 3\n",
    "MAX_LENGTH = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=2)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(2, 1, self.hidden_size, device=device), \n",
    "                torch.zeros(2, 1, self.hidden_size, device=device))\n",
    "    \n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=2)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output[0])\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(2, 1, self.hidden_size, device=device), \n",
    "                torch.zeros(2, 1, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encocer, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        \n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "        \n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        \n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every = 1000, learning_rate = 0.01):\n",
    "    print_loss_total = 0\n",
    "\n",
    "    for iter in range(1, n_iters+1):\n",
    "        training_pair = random.choice(pairs)\n",
    "        input_tensor = tensorFromSentence(word_to_ix, training_pair[0]).to(device)\n",
    "        target_tensor = tensorFromSentence(word_to_ix, training_pair[1]).to(device)\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print(f'Iteration: {iter}, Loss: {print_loss_avg: .4f}')\n",
    "            print_loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length = MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(word_to_ix, sentence).to(device)\n",
    "        input_length = input_tensor.size(0)\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_hidden = tuple([e.to(device) for e in encoder_hidden])\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_words = [] # output sentence\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoder_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoder_words.append(ix_to_word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        \n",
    "        return ' '.join(decoder_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 챗봇\n",
    "def chat(encoder, decoder, max_length = MAX_LENGTH):\n",
    "    print(\"Let's Chat! (type 'bye' to exit)\")\n",
    "    while True:\n",
    "        input_sentence = input('You: ')\n",
    "        print(f'You: {input_sentence}')\n",
    "        if input_sentence == 'bye':\n",
    "            break\n",
    "\n",
    "        output_sentence = evaluate(encoder, decoder, input_sentence)\n",
    "        print(f'Bot: {output_sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/chatbot_dataset.txt', sep = '\\t', names = ['Question', 'Answer'])\n",
    "df['Encoder Inputs'] = df['Question'].apply(clean_text)\n",
    "df['Decoder Inputs'] = df['Answer'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               hi , how are you doing ?\n",
       "1                      i ' m fine . how about yourself ?\n",
       "2                i ' m pretty good . thanks for asking .\n",
       "3                    no problem . so how have you been ?\n",
       "4                   i ' ve been great . what about you ?\n",
       "                             ...                        \n",
       "295             how long have you known how to do that ?\n",
       "296        i first learned how to do it in high school .\n",
       "297    did you take some sort of art class or somethi...\n",
       "298                         that was my favorite class .\n",
       "299                        you have got to be talented .\n",
       "Name: Encoder Inputs, Length: 300, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Encoder Inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = [sentence for sentence in df['Encoder Inputs']]\n",
    "output_sentence = [sentence + \"<EOS>\" for sentence in df['Decoder Inputs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i ' m fine . how about yourself ?<EOS>\",\n",
       " \"i ' m pretty good . thanks for asking .<EOS>\",\n",
       " 'no problem . so how have you been ?<EOS>',\n",
       " \"i ' ve been great . what about you ?<EOS>\",\n",
       " \"i ' ve been good . i ' m in school right now .<EOS>\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sentence[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 사전 생성\n",
    "all_words = set(' '.join(df['Encoder Inputs'].tolist()+df['Decoder Inputs'].tolist()).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'\",\n",
       " ',',\n",
       " '.',\n",
       " '?',\n",
       " 'a',\n",
       " 'about',\n",
       " 'absolutely',\n",
       " 'activities',\n",
       " 'actually',\n",
       " 'after',\n",
       " 'again',\n",
       " 'ago',\n",
       " 'ahead',\n",
       " 'air',\n",
       " 'alice',\n",
       " 'all',\n",
       " 'already',\n",
       " 'always',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'anita',\n",
       " 'answered',\n",
       " 'any',\n",
       " 'anything',\n",
       " 'apologize',\n",
       " 'appreciate',\n",
       " 'are',\n",
       " 'around',\n",
       " 'art',\n",
       " 'asking',\n",
       " 'at',\n",
       " 'attend',\n",
       " 'attending',\n",
       " 'available',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'badly',\n",
       " 'be',\n",
       " 'beach',\n",
       " 'beautiful',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'believe',\n",
       " 'better',\n",
       " 'big',\n",
       " 'bismol',\n",
       " 'boss',\n",
       " 'bothering',\n",
       " 'bought',\n",
       " 'brand',\n",
       " 'brown',\n",
       " 'bumped',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'buy',\n",
       " 'california',\n",
       " 'call',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'campus',\n",
       " 'can',\n",
       " 'change',\n",
       " 'changing',\n",
       " 'chores',\n",
       " 'chuck',\n",
       " 'chucks',\n",
       " 'class',\n",
       " 'classes',\n",
       " 'clean',\n",
       " 'cleaner',\n",
       " 'cleaning',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'clears',\n",
       " 'closer',\n",
       " 'cold',\n",
       " 'come',\n",
       " 'congratulations',\n",
       " 'considering',\n",
       " 'constantly',\n",
       " 'cool',\n",
       " 'cost',\n",
       " 'could',\n",
       " 'couple',\n",
       " 'cute',\n",
       " 'd',\n",
       " 'day',\n",
       " 'days',\n",
       " 'deal',\n",
       " 'degree',\n",
       " 'degrees',\n",
       " 'describe',\n",
       " 'deserved',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'different',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doing',\n",
       " 'dollars',\n",
       " 'don',\n",
       " 'down',\n",
       " 'draw',\n",
       " 'drawing',\n",
       " 'earlier',\n",
       " 'easier',\n",
       " 'enjoy',\n",
       " 'enjoying',\n",
       " 'especially',\n",
       " 'even',\n",
       " 'every',\n",
       " 'everything',\n",
       " 'exactly',\n",
       " 'excited',\n",
       " 'eyes',\n",
       " 'facial',\n",
       " 'far',\n",
       " 'favorite',\n",
       " 'features',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'feet',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'forecast',\n",
       " 'forty',\n",
       " 'found',\n",
       " 'fresh',\n",
       " 'from',\n",
       " 'fun',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'go',\n",
       " 'going',\n",
       " 'good',\n",
       " 'goodbye',\n",
       " 'got',\n",
       " 'great',\n",
       " 'had',\n",
       " 'hang',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happy',\n",
       " 'has',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'height',\n",
       " 'hello',\n",
       " 'her',\n",
       " 'hi',\n",
       " 'hidden',\n",
       " 'high',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'hopefully',\n",
       " 'horrible',\n",
       " 'hot',\n",
       " 'how',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'idea',\n",
       " 'if',\n",
       " 'impossible',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " 'it',\n",
       " 'job',\n",
       " 'just',\n",
       " 'kind',\n",
       " 'kinds',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'known',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'learn',\n",
       " 'learned',\n",
       " 'let',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'little',\n",
       " 'll',\n",
       " 'long',\n",
       " 'look',\n",
       " 'looks',\n",
       " 'lot',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'lovely',\n",
       " 'luck',\n",
       " 'm',\n",
       " 'macy',\n",
       " 'make',\n",
       " 'mall',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'medicine',\n",
       " 'met',\n",
       " 'middle',\n",
       " 'might',\n",
       " 'mind',\n",
       " 'minute',\n",
       " 'missing',\n",
       " 'more',\n",
       " 'most',\n",
       " 'movie',\n",
       " 'much',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'need',\n",
       " 'never',\n",
       " 'new',\n",
       " 'news',\n",
       " 'next',\n",
       " 'nice',\n",
       " 'nicer',\n",
       " 'night',\n",
       " 'ninety',\n",
       " 'no',\n",
       " 'not',\n",
       " 'noticed',\n",
       " 'now',\n",
       " 'of',\n",
       " 'off',\n",
       " 'offered',\n",
       " 'oh',\n",
       " 'okay',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'out',\n",
       " 'outfit',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'own',\n",
       " 'paint',\n",
       " 'painting',\n",
       " 'pair',\n",
       " 'pcc',\n",
       " 'people',\n",
       " 'pepto',\n",
       " 'perfect',\n",
       " 'perfectly',\n",
       " 'phone',\n",
       " 'picked',\n",
       " 'plan',\n",
       " 'planned',\n",
       " 'plans',\n",
       " 'please',\n",
       " 'pointless',\n",
       " 'predictable',\n",
       " 'prettiest',\n",
       " 'pretty',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'promotion',\n",
       " 'quite',\n",
       " 'rain',\n",
       " 'rained',\n",
       " 'raining',\n",
       " 'rains',\n",
       " 'rather',\n",
       " 're',\n",
       " 'real',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'recently',\n",
       " 'right',\n",
       " 'ruin',\n",
       " 's',\n",
       " 'same',\n",
       " 'santa',\n",
       " 'say',\n",
       " 'says',\n",
       " 'school',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'she',\n",
       " 'shoes',\n",
       " 'short',\n",
       " 'shouldn',\n",
       " 'sick',\n",
       " 'since',\n",
       " 'sky',\n",
       " 'smell',\n",
       " 'smells',\n",
       " 'so',\n",
       " 'some',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'sooner',\n",
       " 'sorry',\n",
       " 'sort',\n",
       " 'sounds',\n",
       " 'spare',\n",
       " 'speak',\n",
       " 'speaking',\n",
       " 'special',\n",
       " 'star',\n",
       " 'stars',\n",
       " 'started',\n",
       " 'stay',\n",
       " 'stayed',\n",
       " 'stays',\n",
       " 'still',\n",
       " 'stomach',\n",
       " 'stomachache',\n",
       " 'store',\n",
       " 'such',\n",
       " 'summer',\n",
       " 'supposed',\n",
       " 'sure',\n",
       " 't',\n",
       " 'take',\n",
       " 'talent',\n",
       " 'talented',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'tall',\n",
       " 'taylors',\n",
       " 'tell',\n",
       " 'telling',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'that',\n",
       " 'the',\n",
       " 'them',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinking',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'time',\n",
       " 'times',\n",
       " 'to',\n",
       " 'today',\n",
       " 'told',\n",
       " 'tomorrow',\n",
       " 'too',\n",
       " 'took',\n",
       " 'trip',\n",
       " 'true',\n",
       " 'truth',\n",
       " 'trying',\n",
       " 'ugly',\n",
       " 'uncertain',\n",
       " 'under',\n",
       " 'understand',\n",
       " 'unpredictable',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upset',\n",
       " 'us',\n",
       " 've',\n",
       " 'very',\n",
       " 'wait',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'warm',\n",
       " 'was',\n",
       " 'wasn',\n",
       " 'way',\n",
       " 'we',\n",
       " 'wearing',\n",
       " 'weather',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'weird',\n",
       " 'well',\n",
       " 'went',\n",
       " 'were',\n",
       " 'weren',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'why',\n",
       " 'will',\n",
       " 'winter',\n",
       " 'wish',\n",
       " 'with',\n",
       " 'work',\n",
       " 'would',\n",
       " 'wouldn',\n",
       " 'wrong',\n",
       " 'yeah',\n",
       " 'yes',\n",
       " 'yesterday',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yourself'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {'<PAD>': PAD_token, '<SOS>': SOS_token, '<EOS>': EOS_token, '<UNK>': UNK_token}\n",
    "vocab.update({word: idx+4 for idx, word in enumerate(all_words)})\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 단어 사전 저장\n",
    "with open('data/vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<SOS>': 1,\n",
       " '<EOS>': 2,\n",
       " '<UNK>': 3,\n",
       " 'not': 4,\n",
       " 'noticed': 5,\n",
       " 'people': 6,\n",
       " 'clearly': 7,\n",
       " 'rains': 8,\n",
       " 'cool': 9,\n",
       " 'girls': 10,\n",
       " 'a': 11,\n",
       " 'took': 12,\n",
       " 'rain': 13,\n",
       " 'nice': 14,\n",
       " 'had': 15,\n",
       " 'easier': 16,\n",
       " 'hello': 17,\n",
       " 'think': 18,\n",
       " 'from': 19,\n",
       " 'lovely': 20,\n",
       " 'sky': 21,\n",
       " 'prettiest': 22,\n",
       " 'tell': 23,\n",
       " 'painting': 24,\n",
       " 'sooner': 25,\n",
       " 'because': 26,\n",
       " 'yet': 27,\n",
       " 'while': 28,\n",
       " 'fine': 29,\n",
       " 'our': 30,\n",
       " 'about': 31,\n",
       " 'look': 32,\n",
       " 'find': 33,\n",
       " 'bad': 34,\n",
       " 'better': 35,\n",
       " 'plan': 36,\n",
       " 'only': 37,\n",
       " 'back': 38,\n",
       " 'me': 39,\n",
       " 'will': 40,\n",
       " 'attend': 41,\n",
       " 'so': 42,\n",
       " 'fun': 43,\n",
       " 'beautiful': 44,\n",
       " 'considering': 45,\n",
       " 'degree': 46,\n",
       " 'sort': 47,\n",
       " 'these': 48,\n",
       " 'winter': 49,\n",
       " 'campus': 50,\n",
       " 'excited': 51,\n",
       " 'are': 52,\n",
       " 'until': 53,\n",
       " 'how': 54,\n",
       " 'clean': 55,\n",
       " 'change': 56,\n",
       " 'wait': 57,\n",
       " 'hang': 58,\n",
       " 'am': 59,\n",
       " 'apologize': 60,\n",
       " 'exactly': 61,\n",
       " 'says': 62,\n",
       " 'bismol': 63,\n",
       " 'somewhere': 64,\n",
       " 'feet': 65,\n",
       " 'week': 66,\n",
       " 'down': 67,\n",
       " 'did': 68,\n",
       " 'when': 69,\n",
       " 'what': 70,\n",
       " 'ninety': 71,\n",
       " 'which': 72,\n",
       " 'liked': 73,\n",
       " 'real': 74,\n",
       " 'earlier': 75,\n",
       " 'learned': 76,\n",
       " 'uncertain': 77,\n",
       " 'of': 78,\n",
       " 'constantly': 79,\n",
       " 'boss': 80,\n",
       " 'outside': 81,\n",
       " 'if': 82,\n",
       " 'store': 83,\n",
       " 'appreciate': 84,\n",
       " 'life': 85,\n",
       " 'pair': 86,\n",
       " 'might': 87,\n",
       " 'her': 88,\n",
       " 'missing': 89,\n",
       " 'serious': 90,\n",
       " 'ago': 91,\n",
       " 'home': 92,\n",
       " 'night': 93,\n",
       " 'say': 94,\n",
       " 'talking': 95,\n",
       " 'like': 96,\n",
       " 'happen': 97,\n",
       " 'bought': 98,\n",
       " 'it': 99,\n",
       " 'make': 100,\n",
       " 'thanks': 101,\n",
       " 'closer': 102,\n",
       " 'warm': 103,\n",
       " 'degrees': 104,\n",
       " 'yeah': 105,\n",
       " 'seem': 106,\n",
       " 'hopefully': 107,\n",
       " 'stars': 108,\n",
       " 'kind': 109,\n",
       " 'soon': 110,\n",
       " 'oh': 111,\n",
       " 'unpredictable': 112,\n",
       " 'knew': 113,\n",
       " 'santa': 114,\n",
       " 'on': 115,\n",
       " 'after': 116,\n",
       " 'chucks': 117,\n",
       " 'have': 118,\n",
       " 'one': 119,\n",
       " 'weren': 120,\n",
       " 'thank': 121,\n",
       " 'speaking': 122,\n",
       " 'just': 123,\n",
       " 'speak': 124,\n",
       " 'my': 125,\n",
       " 'do': 126,\n",
       " 'chuck': 127,\n",
       " 'than': 128,\n",
       " 'hot': 129,\n",
       " 'five': 130,\n",
       " 'height': 131,\n",
       " 'see': 132,\n",
       " 'already': 133,\n",
       " 'asking': 134,\n",
       " 'badly': 135,\n",
       " 'wanted': 136,\n",
       " 'alice': 137,\n",
       " 'with': 138,\n",
       " 'describe': 139,\n",
       " 'anything': 140,\n",
       " 'met': 141,\n",
       " 'absolutely': 142,\n",
       " 'plans': 143,\n",
       " 'is': 144,\n",
       " 'would': 145,\n",
       " 'changing': 146,\n",
       " '.': 147,\n",
       " 'really': 148,\n",
       " 'sometimes': 149,\n",
       " 'cute': 150,\n",
       " 'mind': 151,\n",
       " 'over': 152,\n",
       " 'can': 153,\n",
       " 'especially': 154,\n",
       " 'us': 155,\n",
       " 'everything': 156,\n",
       " 'seriously': 157,\n",
       " 'under': 158,\n",
       " 'now': 159,\n",
       " 'later': 160,\n",
       " 'loved': 161,\n",
       " 'off': 162,\n",
       " 'predictable': 163,\n",
       " 'recently': 164,\n",
       " 'stomachache': 165,\n",
       " 'too': 166,\n",
       " 'phone': 167,\n",
       " 'seen': 168,\n",
       " 'hear': 169,\n",
       " 'rather': 170,\n",
       " 'couple': 171,\n",
       " 'pcc': 172,\n",
       " 'wearing': 173,\n",
       " 'taylors': 174,\n",
       " 'picked': 175,\n",
       " 'could': 176,\n",
       " 'and': 177,\n",
       " 'tomorrow': 178,\n",
       " 'to': 179,\n",
       " 'forecast': 180,\n",
       " 'wrong': 181,\n",
       " 'maybe': 182,\n",
       " 'up': 183,\n",
       " 'telling': 184,\n",
       " 'right': 185,\n",
       " 'quite': 186,\n",
       " 'there': 187,\n",
       " 'goodbye': 188,\n",
       " 'always': 189,\n",
       " 'still': 190,\n",
       " 'this': 191,\n",
       " 'stay': 192,\n",
       " 'beach': 193,\n",
       " 'things': 194,\n",
       " 'stays': 195,\n",
       " 'such': 196,\n",
       " 'brown': 197,\n",
       " 'first': 198,\n",
       " 'know': 199,\n",
       " 'bothering': 200,\n",
       " 'where': 201,\n",
       " 'cost': 202,\n",
       " 'paint': 203,\n",
       " 'mall': 204,\n",
       " 'cold': 205,\n",
       " 'draw': 206,\n",
       " 's': 207,\n",
       " 'in': 208,\n",
       " 'dollars': 209,\n",
       " 'luck': 210,\n",
       " 'feeling': 211,\n",
       " 'stayed': 212,\n",
       " 'your': 213,\n",
       " 'available': 214,\n",
       " 'enjoy': 215,\n",
       " 'deserved': 216,\n",
       " 'talented': 217,\n",
       " 'work': 218,\n",
       " 'smells': 219,\n",
       " 'known': 220,\n",
       " 'told': 221,\n",
       " 'answered': 222,\n",
       " 've': 223,\n",
       " 'forty': 224,\n",
       " 'sounds': 225,\n",
       " 'she': 226,\n",
       " 'wish': 227,\n",
       " 'thing': 228,\n",
       " 'call': 229,\n",
       " 'facial': 230,\n",
       " 'something': 231,\n",
       " 'feel': 232,\n",
       " 'll': 233,\n",
       " 'but': 234,\n",
       " 'long': 235,\n",
       " 'supposed': 236,\n",
       " 'even': 237,\n",
       " 'love': 238,\n",
       " 'that': 239,\n",
       " 'started': 240,\n",
       " 'truth': 241,\n",
       " 'fresh': 242,\n",
       " 'shoes': 243,\n",
       " 'does': 244,\n",
       " 'happy': 245,\n",
       " 'sure': 246,\n",
       " 'hidden': 247,\n",
       " 'days': 248,\n",
       " 'want': 249,\n",
       " 'light': 250,\n",
       " 'those': 251,\n",
       " 'congratulations': 252,\n",
       " 'into': 253,\n",
       " 'no': 254,\n",
       " 'an': 255,\n",
       " 'since': 256,\n",
       " 'short': 257,\n",
       " 'take': 258,\n",
       " 'little': 259,\n",
       " 'california': 260,\n",
       " 'way': 261,\n",
       " 'enjoying': 262,\n",
       " 'isn': 263,\n",
       " 'calling': 264,\n",
       " 'special': 265,\n",
       " 'brand': 266,\n",
       " 'idea': 267,\n",
       " 'be': 268,\n",
       " 'same': 269,\n",
       " 'times': 270,\n",
       " 'promotion': 271,\n",
       " 'lately': 272,\n",
       " 'next': 273,\n",
       " 'much': 274,\n",
       " 'may': 275,\n",
       " 'please': 276,\n",
       " 'all': 277,\n",
       " 'well': 278,\n",
       " 'looks': 279,\n",
       " 'rained': 280,\n",
       " 'busy': 281,\n",
       " 'stomach': 282,\n",
       " 'go': 283,\n",
       " 'minute': 284,\n",
       " 'hundred': 285,\n",
       " 'problem': 286,\n",
       " 'drawing': 287,\n",
       " 'school': 288,\n",
       " 'didn': 289,\n",
       " 'ruin': 290,\n",
       " 'pepto': 291,\n",
       " 'favorite': 292,\n",
       " 'we': 293,\n",
       " 'news': 294,\n",
       " 'before': 295,\n",
       " 'big': 296,\n",
       " 'trying': 297,\n",
       " 'or': 298,\n",
       " 'perfect': 299,\n",
       " 'offered': 300,\n",
       " 'movie': 301,\n",
       " 'weather': 302,\n",
       " 'talent': 303,\n",
       " 'weird': 304,\n",
       " 'why': 305,\n",
       " 'got': 306,\n",
       " 'i': 307,\n",
       " 'cleaner': 308,\n",
       " 'though': 309,\n",
       " 'art': 310,\n",
       " 'who': 311,\n",
       " 'happened': 312,\n",
       " 't': 313,\n",
       " 'don': 314,\n",
       " 'air': 315,\n",
       " 'sorry': 316,\n",
       " 'went': 317,\n",
       " 'doesn': 318,\n",
       " 'come': 319,\n",
       " 'reason': 320,\n",
       " 'summer': 321,\n",
       " 'planned': 322,\n",
       " 'was': 323,\n",
       " 'actually': 324,\n",
       " 're': 325,\n",
       " 'features': 326,\n",
       " 'talk': 327,\n",
       " 'again': 328,\n",
       " 'seems': 329,\n",
       " 'perfectly': 330,\n",
       " 'weekend': 331,\n",
       " 'trip': 332,\n",
       " 'then': 333,\n",
       " 'impossible': 334,\n",
       " 'wasn': 335,\n",
       " 'nicer': 336,\n",
       " 'gets': 337,\n",
       " 'high': 338,\n",
       " 'classes': 339,\n",
       " 'any': 340,\n",
       " 'has': 341,\n",
       " 'clear': 342,\n",
       " 'class': 343,\n",
       " 'going': 344,\n",
       " 'middle': 345,\n",
       " 'smell': 346,\n",
       " 'shouldn': 347,\n",
       " 'sick': 348,\n",
       " 'pointless': 349,\n",
       " 'anita': 350,\n",
       " 'found': 351,\n",
       " 'believe': 352,\n",
       " 'every': 353,\n",
       " 'kinds': 354,\n",
       " 'other': 355,\n",
       " 'hope': 356,\n",
       " 'ugly': 357,\n",
       " 'activities': 358,\n",
       " 'seeing': 359,\n",
       " 'raining': 360,\n",
       " 'buy': 361,\n",
       " 'doing': 362,\n",
       " 'get': 363,\n",
       " 'them': 364,\n",
       " 'today': 365,\n",
       " 'mean': 366,\n",
       " 'been': 367,\n",
       " 'let': 368,\n",
       " 'okay': 369,\n",
       " 'haven': 370,\n",
       " 'clears': 371,\n",
       " 'wouldn': 372,\n",
       " 'myself': 373,\n",
       " 'pretty': 374,\n",
       " 'd': 375,\n",
       " 'spare': 376,\n",
       " 'never': 377,\n",
       " 'understand': 378,\n",
       " 'true': 379,\n",
       " 'ahead': 380,\n",
       " 'probably': 381,\n",
       " 'chores': 382,\n",
       " 'bumped': 383,\n",
       " 'called': 384,\n",
       " 'learn': 385,\n",
       " 'deal': 386,\n",
       " 'lot': 387,\n",
       " 'far': 388,\n",
       " 'very': 389,\n",
       " 'the': 390,\n",
       " 'time': 391,\n",
       " 'eyes': 392,\n",
       " 'hi': 393,\n",
       " 'good': 394,\n",
       " 'heard': 395,\n",
       " \"'\": 396,\n",
       " 'm': 397,\n",
       " 'sometime': 398,\n",
       " 'girl': 399,\n",
       " ',': 400,\n",
       " 'most': 401,\n",
       " 'some': 402,\n",
       " 'new': 403,\n",
       " 'cleaning': 404,\n",
       " 'more': 405,\n",
       " 'great': 406,\n",
       " 'macy': 407,\n",
       " 'outfit': 408,\n",
       " 'for': 409,\n",
       " 'star': 410,\n",
       " 'own': 411,\n",
       " 'yourself': 412,\n",
       " 'attending': 413,\n",
       " 'you': 414,\n",
       " 'medicine': 415,\n",
       " '?': 416,\n",
       " 'yes': 417,\n",
       " 'horrible': 418,\n",
       " 'were': 419,\n",
       " 'job': 420,\n",
       " 'they': 421,\n",
       " 'around': 422,\n",
       " 'out': 423,\n",
       " 'upset': 424,\n",
       " 'yesterday': 425,\n",
       " 'at': 426,\n",
       " 'need': 427,\n",
       " 'once': 428,\n",
       " 'different': 429,\n",
       " 'day': 430,\n",
       " 'thinking': 431,\n",
       " 'tall': 432}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = vocab\n",
    "ix_to_word = {i: word for word, i in word_to_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_ix['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderLSTM(vocab_size, hidden_size).to(device)\n",
    "decoder = DecoderLSTM(hidden_size, vocab_size).to(device)\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.005)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.005)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 생성\n",
    "pairs = [list(x) for x in zip(df['Encoder Inputs'], df['Decoder Inputs'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i ' m fine . how about yourself ?\",\n",
       " \"i ' m pretty good . thanks for asking .\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000, Loss:  3.0642\n",
      "Iteration: 2000, Loss:  2.8006\n",
      "Iteration: 3000, Loss:  2.5807\n",
      "Iteration: 4000, Loss:  2.3017\n",
      "Iteration: 5000, Loss:  1.9613\n",
      "Iteration: 6000, Loss:  1.6375\n",
      "Iteration: 7000, Loss:  1.3488\n",
      "Iteration: 8000, Loss:  1.2294\n",
      "Iteration: 9000, Loss:  1.0958\n",
      "Iteration: 10000, Loss:  0.9555\n",
      "Iteration: 11000, Loss:  0.8563\n",
      "Iteration: 12000, Loss:  0.7075\n",
      "Iteration: 13000, Loss:  0.6445\n",
      "Iteration: 14000, Loss:  0.6656\n",
      "Iteration: 15000, Loss:  0.6687\n"
     ]
    }
   ],
   "source": [
    "# 학습 실험\n",
    "trainIters(encoder, decoder, 15000, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder.state_dict(), 'data/encoder_tmp.pth')\n",
    "torch.save(decoder.state_dict(), 'data/decoder_tmp.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderLSTM(\n",
       "  (embedding): Embedding(433, 256)\n",
       "  (lstm): LSTM(256, 256, num_layers=2)\n",
       "  (out): Linear(in_features=256, out_features=433, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 챗봇 실행\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's Chat! (type 'bye' to exit)\n",
      "You: hello\n",
      "Bot: oh . how much to tall . <EOS>\n",
      "You: how are you?\n",
      "Bot: i really great . <EOS>\n",
      "You: how old are you?\n",
      "Bot: how really great . how . how lot . <EOS>\n",
      "You: what did you eat today?\n",
      "Bot: yes , offered me . <EOS>\n",
      "You: bye\n"
     ]
    }
   ],
   "source": [
    "chat(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
