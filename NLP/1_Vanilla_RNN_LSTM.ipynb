{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '''나라말이 중국과 달라 한자와 서로 통하지 아니하므로, 어리석은 백성들이 말하고자 하는 바가 있어도 끝내 제 뜻을 펴지 못하는 사람이 많다 내가 이를 불쌍히 여겨 새로 스물 여덟 글자를 만드니 사람마다 하여금 쉽게 익혀 날마다 씀에 편하게 하고자 할 따름이다'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data):\n",
    "    data = re.sub(r'[^가-힣\\s]', '', data)\n",
    "    tokens = data.split()\n",
    "    vocab = list(set(tokens))\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "    ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "    return tokens, vocab_size, word_to_ix, ix_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(h_size, vocab_size):\n",
    "    U = np.random.randn(h_size, vocab_size) * 0.01\n",
    "    W = np.random.randn(h_size, h_size) * 0.01\n",
    "    V = np.random.randn(vocab_size, h_size) * 0.01\n",
    "    return U, W, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP\n",
    "def feedforward(inputs, targets, hprev): # hprev: 이전 Hidden State\n",
    "    loss = 0\n",
    "    xs, hs, ps, ys = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    for i in range(seq_len):\n",
    "        xs[i] = np.zeros((vocab_size, 1))\n",
    "        xs[i][inputs[i]] = 1 # one-hot encoding\n",
    "        hs[i] = np.tanh(np.dot(U, xs[i]) + np.dot(W, hs[i-1])) # Hidden State 계산\n",
    "        ys[i] = np.dot(V, hs[i])\n",
    "        ps[i] = np.exp(ys[i]) / np.sum(np.exp(ys[i])) # softmax 계산\n",
    "        loss += -np.log(ps[i][targets[i], 0])\n",
    "    return loss, ps, hs, xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(ps, hs, xs):\n",
    "    # Backward Propagation Through Time (BPTT)\n",
    "    # 처음에 모든 가중치들은 0으로 설정\n",
    "    dV = np.zeros(V.shape)\n",
    "    dW = np.zeros(W.shape)\n",
    "    dU = np.zeros(U.shape)\n",
    "\n",
    "    for i in range(seq_len)[::-1]:\n",
    "        output = np.zeros((vocab_size, 1))\n",
    "        output[targets[i]] = 1\n",
    "        ps[i] = ps[i] - output.reshape(-1, 1)\n",
    "\n",
    "        dV_step_i = ps[i] @ (hs[i].T) # (y_hat - y) @ hs.T - for each step\n",
    "        dV += dV_step_i\n",
    "\n",
    "        delta_recent = (V.T @ ps[i]) * (1 - hs[i] ** 2)\n",
    "\n",
    "        for j in range(i+1)[::-1]:\n",
    "            dW_ij = delta_recent @ hs[j-1].T\n",
    "            dW += dW_ij\n",
    "\n",
    "            dU_ij = delta_recent @ xs[j].reshape(1, -1)\n",
    "            dU += dU_ij\n",
    "\n",
    "            delta_recent = (W.T @ delta_recent) * (1 - hs[j-1] ** 2)\n",
    "        \n",
    "        for d in [dU, dW, dV]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "\n",
    "    return dU, dW, dV, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(word, length):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[word_to_ix[word]] = 1\n",
    "    ixes = []\n",
    "    h = np.zeros((h_size, 1))\n",
    "\n",
    "    for t in range(length):\n",
    "        h = np.tanh(np.dot(U, x) + np.dot(W, h))\n",
    "        y = np.dot(V, h)\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.argmax(p) # 가장 높은 확률의 단어를 선택\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    \n",
    "    prep_words = ' '.join([ix_to_word[ix] for ix in ixes])\n",
    "    return prep_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "h_size = 100\n",
    "seq_len = 3\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, vocab_size, word_to_ix, ix_to_word = data_preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나라말이',\n",
       " '중국과',\n",
       " '달라',\n",
       " '한자와',\n",
       " '서로',\n",
       " '통하지',\n",
       " '아니하므로',\n",
       " '어리석은',\n",
       " '백성들이',\n",
       " '말하고자',\n",
       " '하는',\n",
       " '바가',\n",
       " '있어도',\n",
       " '끝내',\n",
       " '제',\n",
       " '뜻을',\n",
       " '펴지',\n",
       " '못하는',\n",
       " '사람이',\n",
       " '많다',\n",
       " '내가',\n",
       " '이를',\n",
       " '불쌍히',\n",
       " '여겨',\n",
       " '새로',\n",
       " '스물',\n",
       " '여덟',\n",
       " '글자를',\n",
       " '만드니',\n",
       " '사람마다',\n",
       " '하여금',\n",
       " '쉽게',\n",
       " '익혀',\n",
       " '날마다',\n",
       " '씀에',\n",
       " '편하게',\n",
       " '하고자',\n",
       " '할',\n",
       " '따름이다']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, W, V = init_weights(h_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.50252988e-03, -3.67714625e-03, -3.34210828e-05, ...,\n",
       "        -1.29653317e-02, -1.17891387e-02,  5.09585015e-03],\n",
       "       [-1.00150819e-03, -1.07953846e-02,  3.01023079e-03, ...,\n",
       "        -4.02864948e-03,  6.88232275e-03,  3.07877789e-03],\n",
       "       [-3.65943042e-03, -1.82963063e-02, -9.58025665e-03, ...,\n",
       "         4.39615447e-03, -1.14761029e-03, -1.05240217e-02],\n",
       "       ...,\n",
       "       [ 1.45902631e-02, -1.62929595e-03,  5.74625118e-03, ...,\n",
       "        -1.33285618e-02,  4.98080517e-04,  1.52250661e-02],\n",
       "       [ 5.98281381e-03,  8.14329480e-03, -1.71042616e-02, ...,\n",
       "         5.96537100e-03,  1.29977773e-02,  1.98031474e-02],\n",
       "       [-1.27315087e-03, -6.33662520e-03, -9.96767504e-03, ...,\n",
       "         1.12440736e-02, -6.85301754e-03,  7.18290946e-03]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 10.989632194500821\n",
      "Epoch: 100, Loss: 2.872124129622122\n",
      "Epoch: 200, Loss: 0.3756356976937405\n",
      "Epoch: 300, Loss: 0.1743231949969733\n",
      "Epoch: 400, Loss: 0.10975351178868817\n",
      "Epoch: 500, Loss: 0.08031774632046923\n",
      "Epoch: 600, Loss: 0.06419006421472803\n",
      "Epoch: 700, Loss: 0.05348128018233771\n",
      "Epoch: 800, Loss: 0.04527186480421673\n",
      "Epoch: 900, Loss: 0.0388750068164998\n",
      "Epoch: 1000, Loss: 0.03391673147266552\n",
      "Epoch: 1100, Loss: 0.030036738385053153\n",
      "Epoch: 1200, Loss: 0.026936534610948908\n",
      "Epoch: 1300, Loss: 0.02439685523776114\n",
      "Epoch: 1400, Loss: 0.022266367232185606\n",
      "Epoch: 1500, Loss: 0.0204426882787836\n",
      "Epoch: 1600, Loss: 0.01885718284019442\n",
      "Epoch: 1700, Loss: 0.01746441099207391\n",
      "Epoch: 1800, Loss: 0.0162334435072776\n",
      "Epoch: 1900, Loss: 0.0151412529611402\n",
      "Epoch: 2000, Loss: 0.01416906391610472\n",
      "Epoch: 2100, Loss: 0.01330087590533279\n",
      "Epoch: 2200, Loss: 0.012522940074742354\n",
      "Epoch: 2300, Loss: 0.011823482904942829\n",
      "Epoch: 2400, Loss: 0.011192441209060893\n",
      "Epoch: 2500, Loss: 0.010621195315575557\n",
      "Epoch: 2600, Loss: 0.010102337371040156\n",
      "Epoch: 2700, Loss: 0.00962949245361241\n",
      "Epoch: 2800, Loss: 0.009197186159889997\n",
      "Epoch: 2900, Loss: 0.008800742505693075\n",
      "Epoch: 3000, Loss: 0.008436196879866156\n",
      "Epoch: 3100, Loss: 0.008100213444807925\n",
      "Epoch: 3200, Loss: 0.0077900007959929365\n",
      "Epoch: 3300, Loss: 0.007503222903966852\n",
      "Epoch: 3400, Loss: 0.00723790516821017\n",
      "Epoch: 3500, Loss: 0.006992339159241902\n",
      "Epoch: 3600, Loss: 0.006764994913962069\n",
      "Epoch: 3700, Loss: 0.00655445516161788\n",
      "Epoch: 3800, Loss: 0.006359387791425182\n",
      "Epoch: 3900, Loss: 0.006178565480867745\n",
      "Epoch: 4000, Loss: 0.006010919444161935\n",
      "Epoch: 4100, Loss: 0.005855577789804274\n",
      "Epoch: 4200, Loss: 0.005711797855913227\n",
      "Epoch: 4300, Loss: 0.005578682394445742\n",
      "Epoch: 4400, Loss: 0.005454668755042431\n",
      "Epoch: 4500, Loss: 0.005337389191241174\n",
      "Epoch: 4600, Loss: 0.005228760665268794\n",
      "Epoch: 4700, Loss: 0.00518711501161926\n",
      "Epoch: 4800, Loss: 0.005204549354896462\n",
      "Epoch: 4900, Loss: 2.2925115375245824\n",
      "Epoch: 5000, Loss: 0.022174324690511213\n",
      "Epoch: 5100, Loss: 0.014639270517259752\n",
      "Epoch: 5200, Loss: 0.01127420585624411\n",
      "Epoch: 5300, Loss: 0.009318230649685566\n",
      "Epoch: 5400, Loss: 0.008050530295946887\n",
      "Epoch: 5500, Loss: 0.0071698021356886976\n",
      "Epoch: 5600, Loss: 0.00652648101013013\n",
      "Epoch: 5700, Loss: 0.006037210993896716\n",
      "Epoch: 5800, Loss: 0.005651861820357892\n",
      "Epoch: 5900, Loss: 0.005338744863159996\n",
      "Epoch: 6000, Loss: 0.005077102608100469\n",
      "Epoch: 6100, Loss: 0.004852968809283946\n",
      "Epoch: 6200, Loss: 0.004656758159739267\n",
      "Epoch: 6300, Loss: 0.004481797520505775\n",
      "Epoch: 6400, Loss: 0.00432339174794936\n",
      "Epoch: 6500, Loss: 0.004178206651082715\n",
      "Epoch: 6600, Loss: 0.004043849945551353\n",
      "Epoch: 6700, Loss: 0.003918582259845083\n",
      "Epoch: 6800, Loss: 0.0038011169038008772\n",
      "Epoch: 6900, Loss: 0.0036904814563202635\n",
      "Epoch: 7000, Loss: 0.0035859226058576736\n",
      "Epoch: 7100, Loss: 0.003486841101438704\n",
      "Epoch: 7200, Loss: 0.003392747473421929\n",
      "Epoch: 7300, Loss: 0.0033032319414584704\n",
      "Epoch: 7400, Loss: 0.0032179439312963127\n",
      "Epoch: 7500, Loss: 0.003136578057413747\n",
      "Epoch: 7600, Loss: 0.0030588644375443078\n",
      "Epoch: 7700, Loss: 0.0029845619024007975\n",
      "Epoch: 7800, Loss: 0.002913453139227886\n",
      "Epoch: 7900, Loss: 0.002845341128600751\n",
      "Epoch: 8000, Loss: 0.0027800464490195477\n",
      "Epoch: 8100, Loss: 0.0027174051675326488\n",
      "Epoch: 8200, Loss: 0.0026572671303421937\n",
      "Epoch: 8300, Loss: 0.002599494531109869\n",
      "Epoch: 8400, Loss: 0.002543960677011609\n",
      "Epoch: 8500, Loss: 0.0024905489006997556\n",
      "Epoch: 8600, Loss: 0.0024391515849198626\n",
      "Epoch: 8700, Loss: 0.0023896692786544013\n",
      "Epoch: 8800, Loss: 0.002342009891588366\n",
      "Epoch: 8900, Loss: 0.0022960879585763616\n",
      "Epoch: 9000, Loss: 0.0022518239688603535\n",
      "Epoch: 9100, Loss: 0.0022091437564021664\n",
      "Epoch: 9200, Loss: 0.002167977948544169\n",
      "Epoch: 9300, Loss: 0.002128261470477131\n",
      "Epoch: 9400, Loss: 0.0020899331029002764\n",
      "Epoch: 9500, Loss: 0.0020529350900432165\n",
      "Epoch: 9600, Loss: 0.002017212794911627\n",
      "Epoch: 9700, Loss: 0.001982714398313874\n",
      "Epoch: 9800, Loss: 0.0019493906379934442\n",
      "Epoch: 9900, Loss: 0.0019171945840496156\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "hprev = np.zeros((h_size, 1))\n",
    "for epoch in range(epochs):\n",
    "    for p in range(len(tokens) - seq_len):\n",
    "        inputs = [word_to_ix[tok] for tok in tokens[p:p + seq_len]] # 나라의 \n",
    "        targets = [word_to_ix[tok] for tok in tokens[p + 1:p + seq_len + 1]] # 말이\n",
    "\n",
    "        loss, ps, hs, xs = feedforward(inputs, targets, hprev)\n",
    "\n",
    "        dU, dW, dV, hprev = backward(ps, hs, xs) # Backward Propagation\n",
    "\n",
    "        W -= learning_rate * dW\n",
    "        U -= learning_rate * dU\n",
    "        V -= learning_rate * dV\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try Again!\n",
      "중국과 달라 한자와 달라 한자와 서로 한자와 서로 통하지 서로 통하지 아니하므로 어리석은 아니하므로 어리석은 백성들이 어리석은 백성들이 말하고자 백성들이\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"input word: \") # 나라말이\n",
    "        if user_input == 'break':\n",
    "            break\n",
    "        response = predict(user_input, 20)\n",
    "        print(response)\n",
    "\n",
    "    except:\n",
    "        print('Try Again!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(input):\n",
    "    return 1 / (1 + np.exp(-input))\n",
    "\n",
    "def sigmoid_derivative(input):\n",
    "    return input * (1 - input)\n",
    "\n",
    "def tanh(input, derivative = False):\n",
    "    return np.tanh(input)\n",
    "\n",
    "def tanh_derivative(input):\n",
    "    return 1 - input ** 2\n",
    "\n",
    "def softmax(input):\n",
    "    return np.exp(input) / np.sum(np.exp(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_epochs, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        # Forget Gate\n",
    "        self.Wf = np.random.randn(hidden_size, input_size) * 0.1\n",
    "        self.bf = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Input Gate\n",
    "        self.Wi = np.random.randn(hidden_size, input_size) * 0.1\n",
    "        self.bi = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Candidate Gate\n",
    "        self.Wc = np.random.randn(hidden_size, input_size) * 0.1\n",
    "        self.bc = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # Output Gate\n",
    "        self.Wo = np.random.randn(hidden_size, input_size) * 0.1\n",
    "        self.bo = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Final Gate\n",
    "        self.Wy = np.random.randn(output_size, hidden_size)\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "    \n",
    "    def reset(self): # Epoch 마다 초기화\n",
    "        self.X = {}\n",
    "\n",
    "        self.HS = {-1: np.zeros((self.hidden_size, 1))}\n",
    "        self.CS = {-1: np.zeros((self.hidden_size, 1))}\n",
    "\n",
    "        self.C = {}\n",
    "        self.O = {}\n",
    "        self.F = {}\n",
    "        self.I = {}\n",
    "        self.outputs = {}\n",
    "\n",
    "    # Forward Propagation\n",
    "    def forward(self, inputs):\n",
    "        x = {}\n",
    "        outputs = []\n",
    "        for t in range(len(inputs)):\n",
    "            x[t] = np.zeros((vocab_size, 1))\n",
    "            x[t][inputs[t]] = 1 # one-hot encoding\n",
    "            self.X[t] = np.concatenate((self.HS[t-1], x[t]))\n",
    "\n",
    "            self.F[t] = sigmoid(np.dot(self.Wf, self.X[t]) + self.bf)\n",
    "            self.I[t] = sigmoid(np.dot(self.Wi, self.X[t]) + self.bi)\n",
    "            self.C[t] = tanh(np.dot(self.Wc, self.X[t]) + self.bc)\n",
    "            self.O[t] = sigmoid(np.dot(self.Wo, self.X[t]) + self.bo)\n",
    "\n",
    "            self.CS[t] = self.F[t] * self.CS[t-1] + self.I[t] * self.C[t]\n",
    "            self.HS[t] = self.O[t] * tanh(self.CS[t])\n",
    "\n",
    "            outputs += [np.dot(self.Wy, self.HS[t]) + self.by]\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def backward(self, errors, inputs):\n",
    "        dLdWf, dLdbf = 0, 0\n",
    "        dLdWi, dLdbi = 0, 0\n",
    "        dLdWc, dLdbc = 0, 0\n",
    "        dLdWo, dLdbo = 0, 0\n",
    "        dLdWy, dLdby = 0, 0\n",
    "\n",
    "        dh_next, dc_next = np.zeros_like(self.HS[0]), np.zeros_like(self.CS[0])\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            error = errors[t]\n",
    "\n",
    "            # Final Gate Weights and Biases Errors\n",
    "            dLdWy += np.dot(error, self.HS[t].T)\n",
    "            dLdby += error\n",
    "\n",
    "            # Calculate the hidden layer error\n",
    "            dLdHS = np.dot(self.Wy.T, error) + dh_next\n",
    "\n",
    "            # Output Gate Weights and Biases Errors\n",
    "            dLdO = tanh(self.CS[t]) * dLdHS * sigmoid_derivative(self.O[t])\n",
    "            dLdWo += np.dot(dLdO, inputs[t].T)\n",
    "            dLdbo += dLdO\n",
    "\n",
    "            # Cell State Errors\n",
    "            dLdCS = tanh_derivative(tanh(self.CS[t])) * dLdHS * self.O[t] + dc_next\n",
    "\n",
    "            # Forget Gate Weights and Biases Errors\n",
    "            dLdf = dLdCS * self.CS[t-1] * sigmoid_derivative(self.F[t])\n",
    "            dLdWf += np.dot(dLdf, inputs[t].T)\n",
    "            dLdbf += dLdf\n",
    "\n",
    "            # Input Gate Weights and Biases Errors\n",
    "            dLdi = dLdCS * self.C[t] * sigmoid_derivative(self.I[t])\n",
    "            dLdWi += np.dot(dLdi, inputs[t].T)\n",
    "            dLdbi += dLdi\n",
    "\n",
    "            # Candidate Gate Weights and Biases Errors\n",
    "            dLdc = dLdCS * self.I[t] * tanh_derivative(self.C[t])\n",
    "            dLdWc += np.dot(dLdc, inputs[t].T)\n",
    "            dLdbc += dLdc\n",
    "\n",
    "            # Concatenated Input Errors\n",
    "            d_z = np.dot(self.Wf.T, dLdf) + np.dot(self.Wi.T, dLdi) + np.dot(self.Wc.T, dLdc) + np.dot(self.Wo.T, dLdO)\n",
    "\n",
    "            # Error of Hidden State and Cell State at next time step\n",
    "            dh_next = d_z[:self.hidden_size, :]\n",
    "            dc_next = self.F[t] * dLdCS\n",
    "        \n",
    "        for d_ in (dLdWf, dLdbf, dLdWi, dLdbi, dLdWc, dLdbc, dLdWo, dLdbo, dLdWy, dLdby):\n",
    "            np.clip(d_, -1, 1, out=d_)\n",
    "        \n",
    "        self.Wf += dLdWf * self.learning_rate * (-1)\n",
    "        self.bf += dLdbf * self.learning_rate * (-1)\n",
    "        \n",
    "        self.Wi += dLdWi * self.learning_rate * (-1)\n",
    "        self.bi += dLdbi * self.learning_rate * (-1)\n",
    "\n",
    "        self.Wc += dLdWc * self.learning_rate * (-1)\n",
    "        self.bc += dLdbc * self.learning_rate * (-1)\n",
    "\n",
    "        self.Wo += dLdWo * self.learning_rate * (-1)\n",
    "        self.bo += dLdbo * self.learning_rate * (-1)\n",
    "\n",
    "        self.Wy += dLdWy * self.learning_rate * (-1)\n",
    "        self.by += dLdby * self.learning_rate * (-1)\n",
    "\n",
    "    def train(self, inputs, labels):\n",
    "        for _ in tqdm(range(self.num_epochs)):\n",
    "            self.reset()\n",
    "            input_idx = [Word_to_ix[input] for input in inputs]\n",
    "            predictions = self.forward(input_idx)\n",
    "\n",
    "            errors = []\n",
    "            for t in range(len(predictions)):\n",
    "                errors += [softmax(predictions[t])]\n",
    "                errors[-1][Word_to_ix[labels[t]]] -= 1\n",
    "\n",
    "            self.backward(errors, self.X)\n",
    "\n",
    "    def test(self, inputs, labels):\n",
    "        accuracy = 0\n",
    "        probabilities = self.forward([Word_to_ix[input] for input in inputs])\n",
    "\n",
    "        gt = ''\n",
    "        output = '나라말이 '\n",
    "        for q in range(len(labels)):\n",
    "            prediction = ix_to_Word[np.argmax(softmax(probabilities[q].reshape(-1)))]\n",
    "            gt += inputs[q] + ' '\n",
    "            output += prediction + ' '\n",
    "\n",
    "            if prediction == labels[q]:\n",
    "                accuracy += 1\n",
    "        print('실제값: ', gt)\n",
    "        print('예측값: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 441.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제값:  나라말이 중국과 달라 한자와 서로 통하지 아니하므로 어리석은 백성들이 말하고자 하는 바가 있어도 끝내 제 뜻을 펴지 못하는 사람이 많다 내가 이를 불쌍히 여겨 새로 스물 여덟 글자를 만드니 사람마다 하여금 쉽게 익혀 날마다 씀에 편하게 하고자 할 \n",
      "예측값:  나라말이 중국과 달라 한자와 서로 통하지 아니하므로 어리석은 백성들이 말하고자 하는 바가 있어도 끝내 제 뜻을 펴지 못하는 사람이 많다 내가 이를 불쌍히 여겨 새로 스물 여덟 글자를 만드니 사람마다 하여금 쉽게 익혀 날마다 씀에 편하게 하고자 할 따름이다 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "tokens, vocab_size, Word_to_ix, ix_to_Word = data_preprocessing(data)\n",
    "train_X, train_y = tokens[:-1], tokens[1:]\n",
    "\n",
    "lstm = LSTM(input_size = vocab_size + hidden_size, hidden_size = hidden_size, output_size = vocab_size, num_epochs = 1000, learning_rate = 0.05)\n",
    "\n",
    "# Train\n",
    "lstm.train(train_X, train_y)\n",
    "lstm.test(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
