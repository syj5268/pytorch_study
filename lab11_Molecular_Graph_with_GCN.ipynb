{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Custom pytorch dataset을 정의하여 여러 input 또는 여러 output이 존재하는 경우를 다뤄보기\n",
    "* Graph convolution을 pytorch로 구현하기\n",
    "* (Gated) skip connection을 pytorch로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -LO  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
    "\n",
    "import sys\n",
    "sys.path.append('/usr/local/lib/python3.6/site-packages/')\n",
    "\n",
    "!conda install -y -c rdkit rdkit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o ZINC.smiles https://raw.githubusercontent.com/heartcored98/Standalone-DeepLearning/master/Lec9/ZINC.smiles\n",
    "!curl -o vocab.npy https://raw.githubusercontent.com/heartcored98/Standalone-DeepLearning/master/Lec9/vocab.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.Crippen import MolLogP\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paser = argparse.ArgumentParser()\n",
    "args = paser.parse_args(\"\")\n",
    "args.seed = 123\n",
    "args.val_size = 0.1\n",
    "args.test_size = 0.1\n",
    "args.shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZINC.smiles 파일에 있는 text로 표현된 분자들을 molecular graph 형태로 바꿔줍니다. 이때, node feature matrix는 아래 그림과 같이 각 원자의 symbol degree 등 화학적 특성을 one-hot vector로 나타낸 형태입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ZINC_smiles(file_name, num_mol):\n",
    "    f = open(file_name, 'r')\n",
    "    contents = f.readlines()\n",
    "\n",
    "    smi_list = []\n",
    "    logP_list = []\n",
    "\n",
    "    for i in tqdm_notebook(range(num_mol), desc='Reading Data'):\n",
    "        smi = contents[i].strip()\n",
    "        m = Chem.MolFromSmiles(smi)\n",
    "        smi_list.append(smi)\n",
    "        logP_list.append(MolLogP(m))\n",
    "\n",
    "    logP_list = np.asarray(logP_list).astype(float)\n",
    "\n",
    "    return smi_list, logP_list\n",
    "\n",
    "\n",
    "def smiles_to_onehot(smi_list):\n",
    "    def smiles_to_vector(smiles, vocab, max_length):\n",
    "        while len(smiles) < max_length:\n",
    "            smiles += \" \"\n",
    "        vector = [vocab.index(str(x)) for x in smiles]\n",
    "        one_hot = np.zeros((len(vocab), max_length), dtype=int)\n",
    "        for i, elm in enumerate(vector):\n",
    "            one_hot[elm][i] = 1\n",
    "        return one_hot\n",
    "\n",
    "    vocab = np.load('./vocab.npy')\n",
    "    smi_total = []\n",
    "\n",
    "    for i, smi in tqdm_notebook(enumerate(smi_list), desc='Converting to One Hot'):\n",
    "        smi_onehot = smiles_to_vector(smi, list(vocab), 120)\n",
    "        smi_total.append(smi_onehot)\n",
    "\n",
    "    return np.asarray(smi_total)\n",
    "\n",
    "def convert_to_graph(smiles_list):\n",
    "    adj = []\n",
    "    adj_norm = []\n",
    "    features = []\n",
    "    maxNumAtoms = 50\n",
    "    for i in tqdm_notebook(smiles_list, desc='Converting to Graph'):\n",
    "        # Mol\n",
    "        iMol = Chem.MolFromSmiles(i.strip())\n",
    "        #Adj\n",
    "        iAdjTmp = Chem.rdmolops.GetAdjacencyMatrix(iMol)\n",
    "        # Feature\n",
    "        if( iAdjTmp.shape[0] <= maxNumAtoms):\n",
    "            # Feature-preprocessing\n",
    "            iFeature = np.zeros((maxNumAtoms, 58))\n",
    "            iFeatureTmp = []\n",
    "            for atom in iMol.GetAtoms():\n",
    "                iFeatureTmp.append( atom_feature(atom) ) ### atom features only\n",
    "            iFeature[0:len(iFeatureTmp), 0:58] = iFeatureTmp ### 0 padding for feature-set\n",
    "            features.append(iFeature)\n",
    "\n",
    "            # Adj-preprocessing\n",
    "            iAdj = np.zeros((maxNumAtoms, maxNumAtoms))\n",
    "            iAdj[0:len(iFeatureTmp), 0:len(iFeatureTmp)] = iAdjTmp + np.eye(len(iFeatureTmp))\n",
    "            adj.append(np.asarray(iAdj))\n",
    "    features = np.asarray(features)\n",
    "\n",
    "    return features, adj\n",
    "\n",
    "def atom_feature(atom):\n",
    "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n",
    "                                      ['C', 'N', 'O', 'S', 'F', 'H', 'Si', 'P', 'Cl', 'Br',\n",
    "                                       'Li', 'Na', 'K', 'Mg', 'Ca', 'Fe', 'As', 'Al', 'I', 'B',\n",
    "                                       'V', 'Tl', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn',\n",
    "                                       'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'Mn', 'Cr', 'Pt', 'Hg', 'Pb']) +\n",
    "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4]) +\n",
    "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5]) +\n",
    "                    [atom.GetIsAromatic()])    # (40, 6, 5, 6, 1)\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_smi, list_logP = read_ZINC_smiles('ZINC.smiles', 10000)\n",
    "list_feature, list_adj = convert_to_graph(list_smi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNDataset(Dataset):\n",
    "    def __init__(self, list_feature, list_adj, list_logP):\n",
    "        self.list_feature = list_feature\n",
    "        self.list_adj = list_adj\n",
    "        self.list_logP = list_logP\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_feature)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.list_feature[index], self.list_adj[index], self.list_logP[index]\n",
    "\n",
    "\n",
    "def partition(list_feature, list_adj, list_logP, args):\n",
    "    num_total = list_feature.shape[0]\n",
    "    num_train = int(num_total * (1 - args.test_size - args.val_size))\n",
    "    num_val = int(num_total * args.val_size)\n",
    "    num_test = int(num_total * args.test_size)\n",
    "\n",
    "    feature_train = list_feature[:num_train]\n",
    "    adj_train = list_adj[:num_train]\n",
    "    logP_train = list_logP[:num_train]\n",
    "    feature_val = list_feature[num_train:num_train + num_val]\n",
    "    adj_val = list_adj[num_train:num_train + num_val]\n",
    "    logP_val = list_logP[num_train:num_train + num_val]\n",
    "    feature_test = list_feature[num_total - num_test:]\n",
    "    adj_test = list_adj[num_total - num_test:]\n",
    "    logP_test = list_logP[num_total - num_test:]\n",
    "        \n",
    "    train_set = GCNDataset(feature_train, adj_train, logP_train)\n",
    "    val_set = GCNDataset(feature_val, adj_val, logP_val)\n",
    "    test_set = GCNDataset(feature_test, adj_test, logP_test)\n",
    "\n",
    "    partition = {\n",
    "        'train': train_set,\n",
    "        'val': val_set,\n",
    "        'test': test_set\n",
    "    }\n",
    "\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = partition(list_feature, list_adj, list_logP, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GCNLayer: node feature matrix와 adjacency matrix의 list를 받아 graph convolution 연산을 수행하는 module 입니다.\n",
    "* (Gated)SkipConnection: ResNet에서 사용되었던 skip connection technique을 구현한 module 입니다.\n",
    "* GCNBlock: node feature matrix와 adjacency matrix의 list를 받아 원하는 갯수의 GCNLayer를 통과시킨 후, (gated)skip connection을 적용하는 module 입니다.\n",
    "* ReadOut: graph structrure에 permutation invariance를 주기 위하여 linear layer를 거친 뒤 batch 별로 summation하는 module 입니다.\n",
    "* Predictor: ReadOut layer로부터의 graph feature vector로부터 logP value를 예측하기 위한 linear layer module 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, n_atom, act=None, bn=False):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        \n",
    "        self.use_bn = bn\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.bn = nn.BatchNorm1d(n_atom)\n",
    "        self.activation = act\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        out = self.linear(x)\n",
    "        out = torch.matmul(adj, out)\n",
    "        if self.use_bn:\n",
    "            out = self.bn(out)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(SkipConnection, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        \n",
    "    def forward(self, in_x, out_x):\n",
    "        if (self.in_dim != self.out_dim):\n",
    "            in_x = self.linear(in_x)\n",
    "        out = in_x + out_x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedSkipConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GatedSkipConnection, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        self.linear_coef_in = nn.Linear(out_dim, out_dim)\n",
    "        self.linear_coef_out = nn.Linear(out_dim, out_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, in_x, out_x):\n",
    "        if (self.in_dim != self.out_dim):\n",
    "            in_x = self.linear(in_x)\n",
    "        z = self.gate_coefficient(in_x, out_x)\n",
    "        out = torch.mul(z, out_x) + torch.mul(1.0-z, in_x)\n",
    "        return out\n",
    "            \n",
    "    def gate_coefficient(self, in_x, out_x):\n",
    "        x1 = self.linear_coef_in(in_x)\n",
    "        x2 = self.linear_coef_out(out_x)\n",
    "        return self.sigmoid(x1+x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_layer, in_dim, hidden_dim, out_dim, n_atom, bn=True, sc='gsc'):\n",
    "        super(GCNBlock, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(n_layer):\n",
    "            self.layers.append(GCNLayer(in_dim if i==0 else hidden_dim,\n",
    "                                        out_dim if i==n_layer-1 else hidden_dim,\n",
    "                                        n_atom,\n",
    "                                        nn.ReLU() if i!=n_layer-1 else None,\n",
    "                                        bn))\n",
    "        self.relu = nn.ReLU()\n",
    "        if sc=='gsc':\n",
    "            self.sc = GatedSkipConnection(in_dim, out_dim)\n",
    "        elif sc=='sc':\n",
    "            self.sc = SkipConnection(in_dim, out_dim)\n",
    "        elif sc=='no':\n",
    "            self.sc = None\n",
    "        else:\n",
    "            assert False, \"Wrong sc type.\"\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        residual = x\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            out, adj = layer((x if i==0 else out), adj)\n",
    "        if self.sc != None:\n",
    "            out = self.sc(residual, out)\n",
    "        out = self.relu(out)\n",
    "        return out, adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadOut(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, act=None):\n",
    "        super(ReadOut, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim= out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_dim, \n",
    "                                self.out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.activation = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = torch.sum(out, 1)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, act=None):\n",
    "        super(Predictor, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_dim,\n",
    "                                self.out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.activation = act\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(GCNNet, self).__init__()\n",
    "        \n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(args.n_block):\n",
    "            self.blocks.append(GCNBlock(args.n_layer,\n",
    "                                        args.in_dim if i==0 else args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.hidden_dim,\n",
    "                                        args.n_atom,\n",
    "                                        args.bn,\n",
    "                                        args.sc))\n",
    "        self.readout = ReadOut(args.hidden_dim, \n",
    "                               args.pred_dim1,\n",
    "                               act=nn.ReLU())\n",
    "        self.pred1 = Predictor(args.pred_dim1,\n",
    "                               args.pred_dim2,\n",
    "                               act=nn.ReLU())\n",
    "        self.pred2 = Predictor(args.pred_dim2,\n",
    "                               args.pred_dim3,\n",
    "                               act=nn.Tanh())\n",
    "        self.pred3 = Predictor(args.pred_dim3,\n",
    "                               args.out_dim)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            out, adj = block((x if i==0 else out), adj)\n",
    "        out = self.readout(out)\n",
    "        out = self.pred1(out)\n",
    "        out = self.pred2(out)\n",
    "        out = self.pred3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GATLayer -> GCNBlock -> GCNNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_dim, output_dim, num_head):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.num_head = num_head\n",
    "        self.atn_dim = output_dim // num_head\n",
    "\n",
    "        self.linears = nn.ModuleList()\n",
    "        self.correlations = nn.ParameterList()\n",
    "\n",
    "        for i in range(self.num_head):\n",
    "            self.linears.append(nn.Linear(in_dim, self.atn_dim))\n",
    "            correlation = torch.FloatTensor(self.atn_dim, self.atn_dim)\n",
    "            nn.init.xavier_normal_(correlation)\n",
    "            self.correlations.append(nn.Parameter(correlation))\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        heads = list()\n",
    "        for i in range(self.num_head):\n",
    "            x_transformed = self.linears[i](x)\n",
    "            alpha = self.attention_matrix(x_transformed, self.correlations[i], adj)\n",
    "            x_head = torch.matmul(alpha, x_transformed)\n",
    "            heads.append(x_head)\n",
    "        output = torch.cat(heads, dim=2)\n",
    "        return output\n",
    "        \n",
    "    def attention_matrix(self, x_transformed, correlation, adj):\n",
    "        x= torch.einsum('akj,ij->aki', (x_transformed, correlation))\n",
    "        alpha = torch.matmul(x, x_transformed.transpose(1, 2))\n",
    "        alpha = torch.mul(alpha, adj)\n",
    "        alpha = self.tanh(alpha)\n",
    "        return alpha        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module): # GCNLayer에서 Attention만 추가된 것\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, n_atom, act=None, bn=False, atn):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        \n",
    "        self.use_bn = bn\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        self.bn = nn.BatchNorm1d(n_atom)\n",
    "        self.activation = act\n",
    "        self.use_atn = atn\n",
    "        self.Attention = Attention(out_dim, out_dim, num_head)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        out = self.linear(x)\n",
    "        if self.use_atn:\n",
    "            out = self.Attention(out, adj)\n",
    "        else:\n",
    "            out = torch.matmul(adj, out)\n",
    "        if self.use_bn:\n",
    "            out = self.bn(out)\n",
    "        if self.activation != None:\n",
    "            out = self.activation(out)\n",
    "        return out, adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, partition, optimizer, criterion, args):\n",
    "    trainloader = torch.utils.data.DataLoader(partition['train'], \n",
    "                                              batch_size=args.train_batch_size, \n",
    "                                              shuffle=True, num_workers=2)\n",
    "    net.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        optimizer.zero_grad() # 매 iteration 마다 실행\n",
    "\n",
    "        # get the inputs\n",
    "        list_feature, list_adj, list_logP = data\n",
    "        list_feature = list_feature.cuda().float()\n",
    "        list_adj = list_adj.cuda().float()\n",
    "        list_logP = list_logP.cuda().float().view(-1, 1)\n",
    "        outputs = net(list_feature, list_adj)\n",
    "\n",
    "        loss = criterion(outputs, list_logP)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    return net, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, partition, criterion, args):\n",
    "    valloader = torch.utils.data.DataLoader(partition['val'], \n",
    "                                            batch_size=args.test_batch_size, \n",
    "                                            shuffle=False, num_workers=2)\n",
    "    net.eval()\n",
    "    val_loss = 0 \n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            list_feature, list_adj, list_logP = data\n",
    "            list_feature = list_feature.cuda().float()\n",
    "            list_adj = list_adj.cuda().float()\n",
    "            list_logP = list_logP.cuda().float().view(-1, 1)\n",
    "            \n",
    "            outputs = net(list_feature, list_adj)\n",
    "\n",
    "            loss = criterion(outputs, list_logP)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss = val_loss / len(valloader)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, partition, args):\n",
    "    testloader = torch.utils.data.DataLoader(partition['test'], \n",
    "                                             batch_size=args.test_batch_size, \n",
    "                                             shuffle=False, num_workers=2)\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        logP_total = list()\n",
    "        pred_logP_total = list()\n",
    "        for data in testloader:\n",
    "            list_feature, list_adj, list_logP = data\n",
    "            list_feature = list_feature.cuda().float()\n",
    "            list_adj = list_adj.cuda().float()\n",
    "            list_logP = list_logP.cuda().float()\n",
    "            logP_total += list_logP.tolist()\n",
    "            list_logP = list_logP.view(-1, 1)\n",
    "            \n",
    "            outputs = net(list_feature, list_adj)\n",
    "            pred_logP_total += outputs.view(-1).tolist()\n",
    "\n",
    "        mae = mean_absolute_error(logP_total, pred_logP_total)\n",
    "        std = np.std(np.array(logP_total)-np.array(pred_logP_total))\n",
    "    \n",
    "    return mae, std, logP_total, pred_logP_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "  \n",
    "    net = GCNNet()\n",
    "    net.cuda()\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "        \n",
    "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
    "        ts = time.time()\n",
    "        net, train_loss = train(net, partition, optimizer, criterion, args)\n",
    "        val_loss = validate(net, partition, criterion, args)\n",
    "        te = time.time()\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
    "        \n",
    "    mae, std, logP_total, pred_logP_total = test(net, partition, args)    \n",
    "    \n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['mae'] = mae\n",
    "    result['std'] = std\n",
    "    result['logP_total'] = logP_total\n",
    "    result['pred_logP_total'] = pred_logP_total\n",
    "    return vars(args), result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 : https://github.com/SeungsuKim/CH485--AI-and-Chemistry/tree/master/Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 1024\n",
    "args.lr = 0.0001\n",
    "args.l2_coef = 0\n",
    "args.optim = 'Adam'\n",
    "args.epoch = 30\n",
    "args.n_block = 2\n",
    "args.n_layer = 2\n",
    "args.n_atom = 50\n",
    "args.in_dim = 58\n",
    "args.hidden_dim = 64\n",
    "args.pred_dim1 = 256\n",
    "args.pred_dim2 = 128\n",
    "args.pred_dim3 = 128\n",
    "args.out_dim = 1\n",
    "args.bn = True\n",
    "args.sc = 'no'\n",
    "args.atn = False\n",
    "args.step_size = 10\n",
    "args.gamma = 0.1\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_lr = [0.01, 0.001, 0.0001, 0.00001]\n",
    "list_n_block = [1, 2, 3]\n",
    "#list_lr = [0.001]\n",
    "#list_n_block = [2]\n",
    "var1 = \"lr\"\n",
    "var2 = \"n_block\"\n",
    "\n",
    "dict_result = dict()\n",
    "n_iter = len(list_n_block)*len(list_lr)*args.epoch*(len(dict_partition['train'])+len(dict_partition['val']))\n",
    "bar = tqdm_notebook(total=n_iter, file=sys.stdout, position=0)\n",
    "\n",
    "for lr in list_lr:\n",
    "    for n_block in list_n_block:\n",
    "        args.lr = lr\n",
    "        args.n_block = n_block\n",
    "        args.exp_name = var1+':'+str(lr)+'/'+var2+':'+str(n_block)\n",
    "        result = vars(experiment(dict_partition, device, bar, args))\n",
    "        print(args.exp_name + \" took \" + str(int(args.time_required)) + \"seconds.\")\n",
    "        dict_result[args.exp_name] = copy.deepcopy(result)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "bar.close()\n",
    "\n",
    "df_result = pd.DataFrame(dict_result).transpose()\n",
    "df_result.to_json('lr vs n_block 50000.JSON', orient='table')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
